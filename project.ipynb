{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use GPU if available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"WewILBCNfyY6D0eTbQ2j\")\n",
    "# project = rf.workspace(\"roboflow-universe-projects\").project(\"fall-detection-ca3o8\")\n",
    "# version = project.version(4)\n",
    "# dataset = version.download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset from yolov8 annotations\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dir, transform=None, target_transform=None, device=device):\n",
    "    self.img_dir = f\"{dir}/images\"\n",
    "    self.label_dir = f\"{dir}/labels\"\n",
    "\n",
    "    # extract file names from the directory, removing the file extension and parent directory\n",
    "    self.img_names = [os.path.splitext(name)[0] for name in os.listdir(self.img_dir)]\n",
    "\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "    self.device = device\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.img_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if isinstance(idx, slice):\n",
    "      return [self.__getitem__(i) for i in range(*idx.indices(len(self)))]\n",
    "    \n",
    "    # construct the file paths for the image and label from the directory, add extension\n",
    "    img_path = os.path.join(self.img_dir, self.img_names[idx] + \".jpg\")\n",
    "    label_path = os.path.join(self.label_dir, self.img_names[idx] + \".txt\")\n",
    "    image = Image.open(img_path)\n",
    "    labels = torch.tensor([[float(l) for l in line.rstrip('\\n').split()] for line in open(label_path)])\n",
    "    if self.transform:\n",
    "      image = self.transform(image)  \n",
    "    if self.target_transform:\n",
    "      labels = self.target_transform(labels)\n",
    "    return image.to(self.device), labels.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(\"Fall-Detection-4/train\", transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute mean and standard deviation of the dataset\n",
    "# concat = torch.cat([dataset[i][0] for i in range(4000)], 0).to(torch.float32)\n",
    "# mean = concat.mean()\n",
    "# std = concat.std()\n",
    "# mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = Compose([ToTensor(), Normalize((0.4379,), (0.3040,))])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = CustomDataset(\"Fall-Detection-4/train\", transform=transform_data)\n",
    "valid_dataset = CustomDataset(\"Fall-Detection-4/valid\", transform=transform_data)\n",
    "test_dataset = CustomDataset(\"Fall-Detection-4/test\", transform=transform_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset[:N], batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset[:N//10], batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset[:N//10], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, test_loader, epochs, criterion):\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in tqdm(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "      if batch_idx % 10 == 9:\n",
    "        print(f\"[{epoch + 1}, {batch_idx + 1}] train loss: {running_loss / 10}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, labels) in tqdm(enumerate(valid_loader, 0), total=len(valid_loader)):\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      if batch_idx % 10 == 9:\n",
    "        print(f\"[{epoch + 1}, {batch_idx + 1}] valid loss: {loss.item()}\")\n",
    "\n",
    "  for batch_idx, (inputs, labels) in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    if batch_idx % 10 == 9:\n",
    "      print(f\"[{epoch + 1}, {batch_idx + 1}] test loss: {loss.item()}\")\n",
    "\n",
    "  print(f\"Final test loss: {loss.item()}\")\n",
    "\n",
    "    \n",
    "  print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride = 1):\n",
    "    super(ConvLayer, self).__init__()\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size // 2)\n",
    "    self.bn = nn.BatchNorm2d(out_channels)\n",
    "    self.relu = nn.ReLU()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "  def __init__(self, in_channels, downsample_channels, out_channels, kernel_size, stride = 1):\n",
    "    super(ConvBlock, self).__init__()\n",
    "    self.conv_layers = nn.Sequential(\n",
    "      ConvLayer(in_channels, downsample_channels, 1, 1),\n",
    "      ConvLayer(downsample_channels, out_channels, kernel_size, stride)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.conv_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLONet(nn.Module):\n",
    "  def __init__(self, s=10, b=2, c=20, ch=3, reg_max=16):\n",
    "    super(YOLONet, self).__init__()\n",
    "\n",
    "    self.s = s\n",
    "    self.nc = c\n",
    "    \n",
    "    self.nl = ch\n",
    "    self.reg_max = reg_max\n",
    "    self.no = c + self.reg_max * 4\n",
    "\n",
    "    self.args = {\n",
    "      \"box\": 7.5, # (float) box loss gain\n",
    "      \"cls\": 0.5, # (float) cls loss gain (scale with pixels)\n",
    "      \"dfl\": 1.5, # (float) dfl loss gain\n",
    "    }\n",
    "\n",
    "    self.conv_layers = nn.Sequential(\n",
    "      ConvLayer(3, 64, 7, 2),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      ConvLayer(64, 192, 3, 1),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      ConvBlock(192, 128, 256, 3, 1),\n",
    "      ConvBlock(256, 256, 512, 3, 1),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      ConvBlock(512, 256, 512, 3, 1),\n",
    "      ConvBlock(512, 256, 512, 3, 1),\n",
    "      ConvBlock(512, 256, 512, 3, 1),\n",
    "      ConvBlock(512, 256, 512, 3, 1),\n",
    "      ConvBlock(512, 512, 1024, 3, 1),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      ConvBlock(1024, 512, 1024, 3, 1),\n",
    "      ConvBlock(1024, 512, 1024, 3, 1),\n",
    "    ) \n",
    "\n",
    "    self.conv_layers2 = nn.Sequential(\n",
    "      ConvLayer(1024, 1024, 3, 1),\n",
    "      ConvLayer(1024, 1024, 3, 2),\n",
    "      ConvLayer(1024, 1024, 3, 1),\n",
    "      ConvLayer(1024, 1024, 3, 1),\n",
    "    )\n",
    "\n",
    "    self.bbox_head = nn.Sequential(\n",
    "      ConvLayer(1024, 1024, 3, 1),\n",
    "      nn.Conv2d(1024, self.reg_max*4, 1, 1)\n",
    "    )\n",
    "\n",
    "    self.cls_head = nn.Sequential(\n",
    "      ConvLayer(1024, 1024, 3, 1),\n",
    "      nn.Conv2d(1024, self.nc, 1, 1)\n",
    "    )\n",
    "\n",
    "    # self.fc_layers = nn.Sequential(\n",
    "    #   nn.Linear(1024 * s * s, 4096),\n",
    "    #   nn.ReLU(),\n",
    "    #   nn.Linear(4096, s * s * reg_max * (b * 5 + c)),\n",
    "    #   nn.ReLU()\n",
    "    # )\n",
    "\n",
    "    self.stride = 16\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv_layers(x)\n",
    "    x = self.conv_layers2(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc_layers(x)\n",
    "    return x\n",
    "  \n",
    "class YOLOPretrainer(nn.Module):\n",
    "  def __init__(self, model: YOLONet, out_features: int):\n",
    "    super(YOLOPretrainer, self).__init__()\n",
    "    self.model = model\n",
    "    self.conv_layers = model.conv_layers\n",
    "\n",
    "    self.avg_pool = nn.AvgPool2d(2, 2)\n",
    "    self.fc = nn.Linear(1024 * model.s * model.s, out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv_layers(x)\n",
    "    out = self.avg_pool(out)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.fc(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLONet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 640, 640])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = yolo(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.5000, 0.7023, 1.0000, 0.5953]],\n",
       "\n",
       "        [[0.0000, 0.5356, 0.7307, 0.9234, 0.5386]],\n",
       "\n",
       "        [[0.0000, 0.5249, 0.6539, 0.6240, 0.3719]],\n",
       "\n",
       "        [[0.0000, 0.3938, 0.6087, 0.4149, 0.4758]],\n",
       "\n",
       "        [[0.0000, 0.2991, 0.7930, 0.5982, 0.4141]],\n",
       "\n",
       "        [[0.0000, 0.5900, 0.7293, 0.2819, 0.4563]],\n",
       "\n",
       "        [[0.0000, 0.5638, 0.7178, 0.8673, 0.5628]],\n",
       "\n",
       "        [[0.0000, 0.5133, 0.6580, 0.2966, 0.5003]],\n",
       "\n",
       "        [[0.0000, 0.3506, 0.6865, 0.4622, 0.3761]],\n",
       "\n",
       "        [[0.0000, 0.6062, 0.8726, 0.6983, 0.2548]],\n",
       "\n",
       "        [[0.0000, 0.4531, 0.2672, 0.9062, 0.5344]],\n",
       "\n",
       "        [[0.0000, 0.5960, 0.7137, 0.8079, 0.4078]],\n",
       "\n",
       "        [[0.0000, 0.4684, 0.5808, 0.1414, 0.3399]],\n",
       "\n",
       "        [[0.0000, 0.6586, 0.3367, 0.6609, 0.2516]],\n",
       "\n",
       "        [[0.0000, 0.5693, 0.4363, 0.3457, 0.3337]],\n",
       "\n",
       "        [[0.0000, 0.6564, 0.7758, 0.4034, 0.3078]],\n",
       "\n",
       "        [[0.0000, 0.5058, 0.5058, 0.8862, 0.9711]],\n",
       "\n",
       "        [[0.0000, 0.1162, 0.7102, 0.2324, 0.2703]],\n",
       "\n",
       "        [[0.0000, 0.2525, 0.7891, 0.4514, 0.3943]],\n",
       "\n",
       "        [[0.0000, 0.4617, 0.7927, 0.2789, 0.4110]],\n",
       "\n",
       "        [[0.0000, 0.6755, 0.6273, 0.5997, 0.4894]],\n",
       "\n",
       "        [[0.0000, 0.7111, 0.4232, 0.4576, 0.5026]],\n",
       "\n",
       "        [[0.0000, 0.5961, 0.7453, 0.6328, 0.3656]],\n",
       "\n",
       "        [[0.0000, 0.7529, 0.6455, 0.4942, 0.3667]],\n",
       "\n",
       "        [[0.0000, 0.4266, 0.5232, 0.3406, 0.4883]],\n",
       "\n",
       "        [[0.0000, 0.6355, 0.7722, 0.4866, 0.4556]],\n",
       "\n",
       "        [[0.0000, 0.4957, 0.6359, 0.5988, 0.4344]],\n",
       "\n",
       "        [[0.0000, 0.3602, 0.5338, 0.5632, 0.9323]],\n",
       "\n",
       "        [[0.0000, 0.5509, 0.7931, 0.7738, 0.3311]],\n",
       "\n",
       "        [[0.0000, 0.7300, 0.7195, 0.3767, 0.5609]],\n",
       "\n",
       "        [[0.0000, 0.5142, 0.6384, 0.4995, 0.7201]],\n",
       "\n",
       "        [[0.0000, 0.9408, 0.7828, 0.1185, 0.4085]]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = yolo(dataset[0][0].to(device).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from v8loss import v8DetectionLoss\n",
    "\n",
    "criterion = v8DetectionLoss(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3000, 84, -1]' is invalid for input of size 3000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, valid_loader, test_loader, epochs, criterion)\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\yapzh\\CodeProjects\\dl-fall-detection\\v8loss.py:467\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    465\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# box, cls, dfl\u001b[39;00m\n\u001b[0;32m    466\u001b[0m feats \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 467\u001b[0m pred_distri, pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mxi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\n\u001b[0;32m    468\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n\u001b[0;32m    471\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    472\u001b[0m pred_distri \u001b[38;5;241m=\u001b[39m pred_distri\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\yapzh\\CodeProjects\\dl-fall-detection\\v8loss.py:467\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    465\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# box, cls, dfl\u001b[39;00m\n\u001b[0;32m    466\u001b[0m feats \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 467\u001b[0m pred_distri, pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mxi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m feats], \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\n\u001b[0;32m    468\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc), \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n\u001b[0;32m    471\u001b[0m pred_scores \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    472\u001b[0m pred_distri \u001b[38;5;241m=\u001b[39m pred_distri\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3000, 84, -1]' is invalid for input of size 3000"
     ]
    }
   ],
   "source": [
    "train(yolo, train_loader, valid_loader, test_loader, 1, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the object classification and bounding box regression model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SSD model\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SSD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
