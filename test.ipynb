{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 7, 7, 30])\n",
      "tensor([0.4963, 0.7682, 0.0885, 0.1320, 0.3074, 0.6341, 0.4901, 0.8964, 0.4556,\n",
      "        0.6323, 0.3489, 0.4017, 0.0223, 0.1689, 0.2939, 0.5185, 0.6977, 0.8000,\n",
      "        0.1610, 0.2823, 0.6816, 0.9152, 0.3971, 0.8742, 0.4194, 0.5529, 0.9527,\n",
      "        0.0362, 0.1852, 0.3734])\n",
      "torch.Size([3, 7, 7, 5])\n",
      "tensor([15.0000,  0.5920,  0.3936,  0.0715,  0.5098])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "N, s, b, c = 10, 7, 2, 20\n",
    "\n",
    "outputs = torch.rand(N, s, s, b*5+c)\n",
    "print(outputs.shape)\n",
    "print(outputs[0, 0, 0, :])\n",
    "\n",
    "labels = torch.rand(3, s, s, 5)\n",
    "labels[:, :, :, 0] = torch.randint(0, c, (3, s, s))\n",
    "print(labels.shape)\n",
    "print(labels[0, 0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(outputs, labels, lambda_coord = 5, lambda_noobj = 0.5):\n",
    "    N, s, b, c = outputs.shape[0], outputs.shape[1], outputs.shape[3]//5-5, outputs.shape[3]-5*(outputs.shape[3]//5)\n",
    "    \n",
    "    xy_loss = torch.sum((outputs[:, :, :, 1:5*b+1:5] - labels[:, :, :, 1:5*b+1:5])**2 + (outputs[:, :, :, 1:5*b+1:5] - labels[:, :, :, 1:5*b+1:5])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 7, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, :, :, 1:5*b+1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/pytorchyolo/utils/loss.py#L58\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-9):\n",
    "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "    box2 = box2.T\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    iou = inter / union\n",
    "    if GIoU or DIoU or CIoU:\n",
    "        # convex (smallest enclosing box) width\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n",
    "                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n",
    "            if DIoU:\n",
    "                return iou - rho2 / c2  # DIoU\n",
    "            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * \\\n",
    "                    torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / ((1 + eps) - iou + v)\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "            c_area = cw * ch + eps  # convex area\n",
    "            return iou - (c_area - union) / c_area  # GIoU\n",
    "    else:\n",
    "        return iou  # IoU\n",
    "\n",
    "def compute_loss(predictions, targets, model):\n",
    "  # Check which device was used\n",
    "  device = targets.device\n",
    "\n",
    "  # Add placeholder varables for the different losses\n",
    "  lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n",
    "\n",
    "  # Build yolo targets\n",
    "  tcls, tbox, indices, anchors = build_targets(predictions, targets, model)  # targets\n",
    "\n",
    "  # Define different loss functions classification\n",
    "  BCEcls = nn.BCEWithLogitsLoss(\n",
    "      pos_weight=torch.tensor([1.0], device=device))\n",
    "  BCEobj = nn.BCEWithLogitsLoss(\n",
    "      pos_weight=torch.tensor([1.0], device=device))\n",
    "\n",
    "  # Calculate losses for each yolo layer\n",
    "  for layer_index, layer_predictions in enumerate(predictions):\n",
    "    # Get image ids, anchors, grid index i and j for each target in the current yolo layer\n",
    "    b, anchor, grid_j, grid_i = indices[layer_index]\n",
    "    # Build empty object target tensor with the same shape as the object prediction\n",
    "    tobj = torch.zeros_like(layer_predictions[..., 0], device=device)  # target obj\n",
    "    # Get the number of targets for this layer.\n",
    "    # Each target is a label box with some scaling and the association of an anchor box.\n",
    "    # Label boxes may be associated to 0 or multiple anchors. So they are multiple times or not at all in the targets.\n",
    "    num_targets = b.shape[0]\n",
    "    # Check if there are targets for this batch\n",
    "    if num_targets:\n",
    "      # Load the corresponding values from the predictions for each of the targets\n",
    "      ps = layer_predictions[b, anchor, grid_j, grid_i]\n",
    "\n",
    "      # Regression of the box\n",
    "      # Apply sigmoid to xy offset predictions in each cell that has a target\n",
    "      pxy = ps[:, :2].sigmoid()\n",
    "      # Apply exponent to wh predictions and multiply with the anchor box that matched best with the label for each cell that has a target\n",
    "      pwh = torch.exp(ps[:, 2:4]) * anchors[layer_index]\n",
    "      # Build box out of xy and wh\n",
    "      pbox = torch.cat((pxy, pwh), 1)\n",
    "      # Calculate CIoU or GIoU for each target with the predicted box for its cell + anchor\n",
    "      iou = bbox_iou(pbox.T, tbox[layer_index], x1y1x2y2=False, CIoU=True)\n",
    "      # We want to minimize our loss so we and the best possible IoU is 1 so we take 1 - IoU and reduce it with a mean\n",
    "      lbox += (1.0 - iou).mean()  # iou loss\n",
    "\n",
    "      # Classification of the objectness\n",
    "      # Fill our empty object target tensor with the IoU we just calculated for each target at the targets position\n",
    "      tobj[b, anchor, grid_j, grid_i] = iou.detach().clamp(0).type(tobj.dtype)  # Use cells with iou > 0 as object targets\n",
    "\n",
    "      # Classification of the class\n",
    "      # Check if we need to do a classification (number of classes > 1)\n",
    "      if ps.size(1) - 5 > 1:\n",
    "        # Hot one class encoding\n",
    "        t = torch.zeros_like(ps[:, 5:], device=device)  # targets\n",
    "        t[range(num_targets), tcls[layer_index]] = 1\n",
    "        # Use the tensor to calculate the BCE loss\n",
    "        lcls += BCEcls(ps[:, 5:], t)  # BCE\n",
    "\n",
    "    # Classification of the objectness the sequel\n",
    "    # Calculate the BCE loss between the on the fly generated target and the network prediction\n",
    "    lobj += BCEobj(layer_predictions[..., 4], tobj) # obj loss\n",
    "\n",
    "  lbox *= 0.05\n",
    "  lobj *= 1.0\n",
    "  lcls *= 0.5\n",
    "\n",
    "  # Merge losses\n",
    "  loss = lbox + lobj + lcls\n",
    "\n",
    "  return loss, torch.cat((lbox, lobj, lcls, loss))\n",
    "\n",
    "\n",
    "def build_targets(p, targets, model):\n",
    "  # Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n",
    "  na, nt = 3, targets.shape[0]  # number of anchors, targets #TODO\n",
    "  tcls, tbox, indices, anch = [], [], [], []\n",
    "  gain = torch.ones(7, device=targets.device)  # normalized to gridspace gain\n",
    "  # Make a tensor that iterates 0-2 for 3 anchors and repeat that as many times as we have target boxes\n",
    "  ai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt)\n",
    "  # Copy target boxes anchor size times and append an anchor index to each copy the anchor index is also expressed by the new first dimension\n",
    "  targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)\n",
    "\n",
    "  for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "      # Scale anchors by the yolo grid cell size so that an anchor with the size of the cell would result in 1\n",
    "      anchors = yolo_layer.anchors / yolo_layer.stride\n",
    "      # Add the number of yolo cells in this layer the gain tensor\n",
    "      # The gain tensor matches the collums of our targets (img id, class, x, y, w, h, anchor id)\n",
    "      gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "      # Scale targets by the number of yolo layer cells, they are now in the yolo cell coordinate system\n",
    "      t = targets * gain\n",
    "      # Check if we have targets\n",
    "      if nt:\n",
    "          # Calculate ration between anchor and target box for both width and height\n",
    "          r = t[:, :, 4:6] / anchors[:, None]\n",
    "          # Select the ratios that have the highest divergence in any axis and check if the ratio is less than 4\n",
    "          j = torch.max(r, 1. / r).max(2)[0] < 4  # compare #TODO\n",
    "          # Only use targets that have the correct ratios for their anchors\n",
    "          # That means we only keep ones that have a matching anchor and we loose the anchor dimension\n",
    "          # The anchor id is still saved in the 7th value of each target\n",
    "          t = t[j]\n",
    "      else:\n",
    "          t = targets[0]\n",
    "\n",
    "      # Extract image id in batch and class id\n",
    "      b, c = t[:, :2].long().T\n",
    "      # We isolate the target cell associations.\n",
    "      # x, y, w, h are allready in the cell coordinate system meaning an x = 1.2 would be 1.2 times cellwidth\n",
    "      gxy = t[:, 2:4]\n",
    "      gwh = t[:, 4:6]  # grid wh\n",
    "      # Cast to int to get an cell index e.g. 1.2 gets associated to cell 1\n",
    "      gij = gxy.long()\n",
    "      # Isolate x and y index dimensions\n",
    "      gi, gj = gij.T  # grid xy indices\n",
    "\n",
    "      # Convert anchor indexes to int\n",
    "      a = t[:, 6].long()\n",
    "      # Add target tensors for this yolo layer to the output lists\n",
    "      # Add to index list and limit index range to prevent out of bounds\n",
    "      indices.append((b, a, gj.clamp_(0, gain[3].long() - 1), gi.clamp_(0, gain[2].long() - 1)))\n",
    "      # Add to target box list and convert box coordinates from global grid coordinates to local offsets in the grid cell\n",
    "      tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "      # Add correct anchor for each target to the list\n",
    "      anch.append(anchors[a])\n",
    "      # Add class for each target to the list\n",
    "      tcls.append(c)\n",
    "\n",
    "  return tcls, tbox, indices, anch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
