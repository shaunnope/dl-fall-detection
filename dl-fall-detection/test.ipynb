{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yapzh\\miniconda3\\envs\\dsenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "\n",
    "from data.dataset import YOLOv8Dataset as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = Compose([ToTensor(), Normalize((0.4379,), (0.3040,))])\n",
    "\n",
    "train_dataset = Dataset('../Human-fall-Detection-5/train', transform=transform_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train, sample_targets = train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 640, 640]),\n",
       " tensor([1, 2, 3, 3, 3, 3], device='cuda:0'),\n",
       " tensor([[0.4305, 0.6102, 0.2523, 0.1281],\n",
       "         [0.6648, 0.6125, 0.2086, 0.1984],\n",
       "         [0.3508, 0.4953, 0.1805, 0.1695],\n",
       "         [0.8953, 0.4945, 0.1352, 0.3367],\n",
       "         [0.7789, 0.4313, 0.1289, 0.3305],\n",
       "         [0.5977, 0.4219, 0.1547, 0.2922]], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train.shape, sample_targets[0], sample_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.model import YOLOHead, YOLONet\n",
    "\n",
    "# model = YOLOHead(ch=(64,16,4))\n",
    "# samples = model([torch.rand(1, 64, 14, 14), torch.rand(1, 16, 14, 14), torch.rand(1, 4, 14, 14)])\n",
    "\n",
    "model = YOLONet(ch=(64,32,32))\n",
    "samples = model(torch.rand(1, 3, 448, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 144, 14, 14])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 32.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(sample_train.to('cpu').unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 144, 40, 40])\n",
      "torch.Size([1, 144, 20, 20])\n",
      "torch.Size([1, 144, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))\n",
    "print(preds[0].shape)\n",
    "print(preds[1].shape)\n",
    "print(preds[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.loss import DetectionLoss\n",
    "\n",
    "loss = DetectionLoss(model.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yapzh\\miniconda3\\envs\\dsenv\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_targets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yapzh\\CodeProjects\\dl-fall-detection\\dl-fall-detection\\model\\loss.py:485\u001b[0m, in \u001b[0;36mDetectionLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    481\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n\u001b[0;32m    484\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m--> 485\u001b[0m     (\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    487\u001b[0m )\n\u001b[0;32m    488\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(\n\u001b[0;32m    489\u001b[0m     targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_size, scale_tensor\u001b[38;5;241m=\u001b[39mimgsz[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    490\u001b[0m )\n\u001b[0;32m    491\u001b[0m gt_labels, gt_bboxes \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# cls, xyxy\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "loss(preds, sample_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[  1.2670,   1.2672,   1.3332,  ...,   1.4719,   1.2848,   1.0882],\n",
       "           [  0.8073,   1.6431,   1.1091,  ...,   1.1172,   1.5834,   1.6442],\n",
       "           [  1.0529,   1.4204,   1.9249,  ...,   2.0203,   1.3417,   1.4216],\n",
       "           ...,\n",
       "           [  1.9050,   1.4959,   2.1015,  ...,   1.9404,   1.4768,   0.9937],\n",
       "           [  1.0937,   1.4174,   1.1882,  ...,   1.9289,   1.5472,   0.9966],\n",
       "           [  1.1591,   1.5806,   1.6865,  ...,   1.1590,   1.0945,   1.7639]],\n",
       " \n",
       "          [[  1.2546,   1.6618,   1.0971,  ...,   1.3074,   1.3790,   1.5993],\n",
       "           [  1.1477,   0.7365,   1.1743,  ...,   1.3670,   1.2540,   1.0040],\n",
       "           [  0.9808,   1.3626,   1.4167,  ...,   1.6329,   1.2057,   1.3081],\n",
       "           ...,\n",
       "           [  1.4615,   1.4841,   1.0966,  ...,   1.8724,   0.8699,   1.4434],\n",
       "           [  0.8014,   1.1895,   1.3852,  ...,   1.5550,   0.8565,   1.0063],\n",
       "           [  1.4069,   0.9768,   1.5330,  ...,   1.0894,   1.3240,   1.2866]],\n",
       " \n",
       "          [[  0.9597,   0.4397,   1.0322,  ...,   0.8177,   0.3717,   1.1791],\n",
       "           [  0.7133,   1.3400,   0.5476,  ...,   0.9701,   0.9514,   0.7533],\n",
       "           [  1.0948,   0.9435,   0.6231,  ...,   1.2268,   1.1547,   0.4847],\n",
       "           ...,\n",
       "           [  0.8208,   1.0078,   0.4889,  ...,   0.7575,   0.7029,   1.1163],\n",
       "           [  0.6957,   1.2140,   0.8415,  ...,   0.9709,   0.5600,   1.0560],\n",
       "           [  0.5160,   1.0035,   0.5132,  ...,   1.1189,   1.0592,   0.7270]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-10.0087, -10.0706, -10.1091,  ...,  -9.7933,  -9.9772,  -9.8533],\n",
       "           [ -9.6215, -10.3366,  -9.5781,  ...,  -9.6591, -10.3866,  -9.5678],\n",
       "           [ -9.8702, -10.3357,  -9.1431,  ...,  -9.7819,  -9.7186,  -9.5679],\n",
       "           ...,\n",
       "           [ -9.6153, -10.2309,  -9.8657,  ...,  -9.7728,  -9.9064,  -9.7445],\n",
       "           [ -9.8488,  -9.7546,  -9.4931,  ...,  -9.8130,  -9.1473,  -9.5779],\n",
       "           [ -9.7407,  -9.5495,  -9.2834,  ...,  -9.7393,  -9.8133,  -9.9280]],\n",
       " \n",
       "          [[ -9.8490,  -9.8858,  -9.8165,  ..., -10.0076,  -9.9497,  -9.6909],\n",
       "           [ -9.8975, -10.1613,  -9.2908,  ..., -10.1664, -10.1740,  -9.8707],\n",
       "           [ -9.7799,  -9.8072, -10.0291,  ..., -10.1276,  -9.5503, -10.1068],\n",
       "           ...,\n",
       "           [ -9.5824,  -9.4305,  -9.8180,  ...,  -9.7199, -10.0090,  -9.8077],\n",
       "           [ -9.9418, -10.1456,  -9.0633,  ..., -10.4268,  -9.8031, -10.2071],\n",
       "           [ -9.3707,  -9.5217,  -9.6053,  ...,  -9.3469,  -9.9596,  -9.8917]],\n",
       " \n",
       "          [[-10.1648,  -9.7993, -10.0382,  ..., -10.1392,  -9.8796,  -9.8376],\n",
       "           [ -9.7255,  -9.7570,  -9.6612,  ...,  -9.8888, -10.1623, -10.2395],\n",
       "           [-10.0841, -10.6055, -10.4743,  ..., -10.3765, -10.2864, -10.3363],\n",
       "           ...,\n",
       "           [ -9.9735, -10.2892, -10.1936,  ..., -10.2985, -10.1382, -10.2658],\n",
       "           [ -9.8688,  -9.8721,  -9.6310,  ..., -10.5679,  -9.7445,  -9.9903],\n",
       "           [-10.0315, -10.1703,  -9.7446,  ..., -10.6011, -10.0756, -10.5302]]]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " tensor([[[[ 1.2250,  0.8418,  1.5971,  ...,  1.4226,  1.6848,  1.3757],\n",
       "           [ 1.3376,  1.2610,  0.6662,  ...,  1.7327,  1.4886,  1.1471],\n",
       "           [ 1.6784,  1.6858,  2.0635,  ...,  1.8891,  1.2939,  1.8385],\n",
       "           ...,\n",
       "           [ 1.7479,  1.6065,  2.5881,  ...,  1.7807,  1.5996,  1.4707],\n",
       "           [ 1.6667,  1.4889,  1.0700,  ...,  2.0718,  1.7177,  1.2714],\n",
       "           [ 1.5345,  1.7697,  1.4244,  ...,  1.5509,  1.3682,  1.1306]],\n",
       " \n",
       "          [[ 1.2927,  1.2472,  0.9122,  ...,  1.6202,  1.2441,  1.2090],\n",
       "           [ 1.2463,  1.1775,  0.7930,  ...,  1.5421,  1.1658,  1.3442],\n",
       "           [ 1.0434,  1.0794,  1.3365,  ...,  1.0301,  1.0644,  0.5332],\n",
       "           ...,\n",
       "           [ 1.2771,  1.1495,  0.8836,  ...,  0.9915,  1.0019,  1.0648],\n",
       "           [ 1.1155,  1.6376,  0.6373,  ...,  0.9875,  1.3173,  1.1390],\n",
       "           [ 1.1396,  1.1301,  1.3903,  ...,  1.0915,  0.8234,  1.5130]],\n",
       " \n",
       "          [[ 1.1807,  0.8311,  1.3218,  ...,  1.4161,  0.6647,  0.6285],\n",
       "           [ 1.3382,  0.9250,  0.9629,  ...,  0.3774,  1.1858,  1.0644],\n",
       "           [ 1.1387,  0.7174,  1.0621,  ...,  0.7778,  0.7860,  0.8013],\n",
       "           ...,\n",
       "           [ 0.6412,  0.9051,  1.2633,  ...,  0.4189,  0.7668,  1.3201],\n",
       "           [ 0.7242,  0.9523,  1.5086,  ...,  1.6639,  1.1566,  1.1793],\n",
       "           [ 1.0918,  0.5192,  1.1942,  ...,  1.6307,  0.6863,  1.0261]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-8.3957, -8.2599, -8.1377,  ..., -8.4458, -8.4793, -8.0810],\n",
       "           [-8.6274, -8.9464, -8.7527,  ..., -9.6007, -8.1720, -8.2850],\n",
       "           [-8.6787, -8.7982, -8.8865,  ..., -8.8604, -8.5623, -8.7170],\n",
       "           ...,\n",
       "           [-8.2821, -8.2220, -8.7222,  ..., -9.1311, -8.5601, -8.3864],\n",
       "           [-8.4173, -8.8799, -8.3577,  ..., -7.8987, -8.8945, -8.6087],\n",
       "           [-8.8581, -8.5401, -8.5301,  ..., -8.8115, -8.7102, -8.6731]],\n",
       " \n",
       "          [[-8.2391, -8.2971, -8.4548,  ..., -8.5400, -8.6039, -8.3653],\n",
       "           [-8.6353, -8.6494, -9.1766,  ..., -9.1407, -8.7234, -9.1215],\n",
       "           [-8.7413, -8.7588, -8.4876,  ..., -8.2914, -8.2548, -8.7120],\n",
       "           ...,\n",
       "           [-7.8572, -8.4396, -8.0930,  ..., -8.7559, -7.7814, -8.7860],\n",
       "           [-8.0734, -7.9427, -8.6604,  ..., -9.3480, -8.7439, -8.6185],\n",
       "           [-7.9476, -8.7065, -7.9415,  ..., -7.9694, -8.8436, -8.2621]],\n",
       " \n",
       "          [[-8.6209, -8.3234, -8.3140,  ..., -8.7073, -8.4106, -8.6553],\n",
       "           [-8.7265, -8.3394, -8.7806,  ..., -8.7929, -8.4102, -9.0109],\n",
       "           [-9.2010, -8.5267, -8.1241,  ..., -8.1432, -8.8691, -8.5426],\n",
       "           ...,\n",
       "           [-8.9402, -7.5766, -8.4920,  ..., -8.7090, -9.2088, -8.7685],\n",
       "           [-8.6896, -8.6743, -8.1330,  ..., -8.6592, -9.2492, -8.8706],\n",
       "           [-9.0573, -9.0607, -8.9842,  ..., -9.1514, -8.7778, -8.9505]]]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " tensor([[[[ 0.8842,  0.2810,  1.1549,  ...,  0.3358,  0.8894,  0.9197],\n",
       "           [ 0.7767,  0.9592,  0.9444,  ...,  0.9682,  0.9285,  1.1685],\n",
       "           [ 1.1963,  0.4232,  0.6128,  ...,  1.3845,  0.7398,  0.3881],\n",
       "           ...,\n",
       "           [ 1.2697,  0.3771,  1.2450,  ...,  1.0140,  0.9455,  1.1685],\n",
       "           [ 1.2851,  0.8495,  1.1523,  ...,  1.9180,  1.2156,  1.1352],\n",
       "           [ 1.2462,  1.2228,  1.4666,  ...,  1.2854,  1.0932,  1.3809]],\n",
       " \n",
       "          [[ 0.7700,  0.8880,  0.9178,  ...,  0.9608,  0.7810,  0.8311],\n",
       "           [ 0.9775,  0.9148,  0.8737,  ...,  1.1315,  0.5465,  0.6018],\n",
       "           [ 0.4473,  0.5825,  0.9714,  ...,  1.0318,  0.7606,  0.7011],\n",
       "           ...,\n",
       "           [ 0.9162,  0.6758,  0.9799,  ...,  1.1947,  0.7847,  0.7707],\n",
       "           [ 0.3884,  0.9251,  0.5114,  ...,  0.3212,  1.3529,  0.5273],\n",
       "           [ 0.7542,  0.7620,  0.3527,  ...,  0.8477,  0.3466,  0.5587]],\n",
       " \n",
       "          [[ 1.0615,  0.8635,  0.8385,  ...,  0.6821,  0.5655,  1.1229],\n",
       "           [ 0.7322,  0.9930,  1.5917,  ...,  0.6895,  0.5138,  1.1976],\n",
       "           [ 1.1395,  0.8517,  0.6925,  ...,  1.0484,  1.1734,  1.2644],\n",
       "           ...,\n",
       "           [ 0.8618,  0.8138,  1.0906,  ...,  0.3199,  0.9886,  0.7432],\n",
       "           [ 1.3587,  0.9051,  1.1491,  ...,  0.3644,  1.3315,  0.7217],\n",
       "           [ 0.9293,  0.8760,  0.7425,  ...,  1.2340,  1.5244,  1.2926]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.4259, -7.7046, -7.2807,  ..., -7.0901, -6.4696, -7.1448],\n",
       "           [-6.6905, -7.6796, -7.6805,  ..., -7.2214, -7.6145, -7.4553],\n",
       "           [-7.0575, -6.8482, -7.5540,  ..., -7.4051, -7.2227, -7.1910],\n",
       "           ...,\n",
       "           [-7.2392, -6.6761, -6.8354,  ..., -6.5608, -7.3190, -7.1527],\n",
       "           [-7.2662, -6.4962, -7.1124,  ..., -7.1316, -7.3451, -7.1055],\n",
       "           [-7.2813, -6.7579, -7.1597,  ..., -6.9889, -7.0511, -6.8523]],\n",
       " \n",
       "          [[-7.3573, -7.3484, -7.4041,  ..., -7.3530, -7.1822, -7.2255],\n",
       "           [-7.3828, -7.6548, -7.4918,  ..., -7.8533, -7.2175, -7.8010],\n",
       "           [-7.3476, -7.3061, -6.8042,  ..., -7.8196, -7.8572, -7.4977],\n",
       "           ...,\n",
       "           [-7.4859, -7.7255, -7.7467,  ..., -7.6787, -7.0666, -7.5611],\n",
       "           [-6.8010, -7.2882, -7.1484,  ..., -6.7353, -7.7713, -7.2364],\n",
       "           [-7.4859, -7.0304, -6.9671,  ..., -7.0679, -7.7698, -7.5249]],\n",
       " \n",
       "          [[-7.5398, -7.3439, -7.3824,  ..., -7.3144, -6.9199, -7.3507],\n",
       "           [-7.4208, -7.5259, -7.4251,  ..., -7.2984, -6.7580, -7.8717],\n",
       "           [-7.7159, -7.2130, -7.7069,  ..., -6.9708, -7.9223, -7.0614],\n",
       "           ...,\n",
       "           [-8.1089, -7.0997, -7.4255,  ..., -6.7934, -7.6492, -7.5501],\n",
       "           [-8.0075, -7.9802, -7.4952,  ..., -7.1209, -7.6524, -7.6417],\n",
       "           [-7.7465, -7.5047, -7.1085,  ..., -7.6952, -7.7693, -8.1271]]]],\n",
       "        grad_fn=<CatBackward0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "\n",
    "idx = torch.ones(N, 1)*4\n",
    "cls = torch.rand(N, 1)\n",
    "box = torch.rand(N, 4)\n",
    "\n",
    "targets = torch.cat(\n",
    "    (idx.view(-1, 1), cls.view(-1, 1), box),\n",
    "    1,\n",
    ")\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = targets[:,0].unique(return_counts=True)[1].to(dtype=torch.int32)\n",
    "out = torch.zeros(16, counts.max(), 5)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.0000, 0.4179, 0.4273, 0.6513, 0.5899, 0.6324]]),\n",
       " tensor([[4.0000, 0.4382, 0.0670, 0.8358, 0.0052, 0.8021],\n",
       "         [4.0000, 0.1560, 0.6904, 0.3744, 0.3639, 0.7049],\n",
       "         [4.0000, 0.8038, 0.7348, 0.8887, 0.0342, 0.7100],\n",
       "         [4.0000, 0.6188, 0.2406, 0.3678, 0.3155, 0.6877]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.split((1,4), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 7, 7, 30])\n",
      "tensor([0.4963, 0.7682, 0.0885, 0.1320, 0.3074, 0.6341, 0.4901, 0.8964, 0.4556,\n",
      "        0.6323, 0.3489, 0.4017, 0.0223, 0.1689, 0.2939, 0.5185, 0.6977, 0.8000,\n",
      "        0.1610, 0.2823, 0.6816, 0.9152, 0.3971, 0.8742, 0.4194, 0.5529, 0.9527,\n",
      "        0.0362, 0.1852, 0.3734])\n",
      "torch.Size([3, 7, 7, 5])\n",
      "tensor([15.0000,  0.5920,  0.3936,  0.0715,  0.5098])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "N, s, b, c = 10, 7, 2, 20\n",
    "\n",
    "outputs = torch.rand(N, s, s, b*5+c)\n",
    "print(outputs.shape)\n",
    "print(outputs[0, 0, 0, :])\n",
    "\n",
    "labels = torch.rand(3, s, s, 5)\n",
    "labels[:, :, :, 0] = torch.randint(0, c, (3, s, s))\n",
    "print(labels.shape)\n",
    "print(labels[0, 0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(outputs, labels, lambda_coord = 5, lambda_noobj = 0.5):\n",
    "    N, s, b, c = outputs.shape[0], outputs.shape[1], outputs.shape[3]//5-5, outputs.shape[3]-5*(outputs.shape[3]//5)\n",
    "    \n",
    "    xy_loss = torch.sum((outputs[:, :, :, 1:5*b+1:5] - labels[:, :, :, 1:5*b+1:5])**2 + (outputs[:, :, :, 1:5*b+1:5] - labels[:, :, :, 1:5*b+1:5])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 7, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, :, :, 1:5*b+1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/pytorchyolo/utils/loss.py#L58\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-9):\n",
    "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
    "    box2 = box2.T\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
    "    else:  # transform from xywh to xyxy\n",
    "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
    "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
    "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
    "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    iou = inter / union\n",
    "    if GIoU or DIoU or CIoU:\n",
    "        # convex (smallest enclosing box) width\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n",
    "                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n",
    "            if DIoU:\n",
    "                return iou - rho2 / c2  # DIoU\n",
    "            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * \\\n",
    "                    torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / ((1 + eps) - iou + v)\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "            c_area = cw * ch + eps  # convex area\n",
    "            return iou - (c_area - union) / c_area  # GIoU\n",
    "    else:\n",
    "        return iou  # IoU\n",
    "\n",
    "def compute_loss(predictions, targets, model):\n",
    "  # Check which device was used\n",
    "  device = targets.device\n",
    "\n",
    "  # Add placeholder varables for the different losses\n",
    "  lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n",
    "\n",
    "  # Build yolo targets\n",
    "  tcls, tbox, indices, anchors = build_targets(predictions, targets, model)  # targets\n",
    "\n",
    "  # Define different loss functions classification\n",
    "  BCEcls = nn.BCEWithLogitsLoss(\n",
    "      pos_weight=torch.tensor([1.0], device=device))\n",
    "  BCEobj = nn.BCEWithLogitsLoss(\n",
    "      pos_weight=torch.tensor([1.0], device=device))\n",
    "\n",
    "  # Calculate losses for each yolo layer\n",
    "  for layer_index, layer_predictions in enumerate(predictions):\n",
    "    # Get image ids, anchors, grid index i and j for each target in the current yolo layer\n",
    "    b, anchor, grid_j, grid_i = indices[layer_index]\n",
    "    # Build empty object target tensor with the same shape as the object prediction\n",
    "    tobj = torch.zeros_like(layer_predictions[..., 0], device=device)  # target obj\n",
    "    # Get the number of targets for this layer.\n",
    "    # Each target is a label box with some scaling and the association of an anchor box.\n",
    "    # Label boxes may be associated to 0 or multiple anchors. So they are multiple times or not at all in the targets.\n",
    "    num_targets = b.shape[0]\n",
    "    # Check if there are targets for this batch\n",
    "    if num_targets:\n",
    "      # Load the corresponding values from the predictions for each of the targets\n",
    "      ps = layer_predictions[b, anchor, grid_j, grid_i]\n",
    "\n",
    "      # Regression of the box\n",
    "      # Apply sigmoid to xy offset predictions in each cell that has a target\n",
    "      pxy = ps[:, :2].sigmoid()\n",
    "      # Apply exponent to wh predictions and multiply with the anchor box that matched best with the label for each cell that has a target\n",
    "      pwh = torch.exp(ps[:, 2:4]) * anchors[layer_index]\n",
    "      # Build box out of xy and wh\n",
    "      pbox = torch.cat((pxy, pwh), 1)\n",
    "      # Calculate CIoU or GIoU for each target with the predicted box for its cell + anchor\n",
    "      iou = bbox_iou(pbox.T, tbox[layer_index], x1y1x2y2=False, CIoU=True)\n",
    "      # We want to minimize our loss so we and the best possible IoU is 1 so we take 1 - IoU and reduce it with a mean\n",
    "      lbox += (1.0 - iou).mean()  # iou loss\n",
    "\n",
    "      # Classification of the objectness\n",
    "      # Fill our empty object target tensor with the IoU we just calculated for each target at the targets position\n",
    "      tobj[b, anchor, grid_j, grid_i] = iou.detach().clamp(0).type(tobj.dtype)  # Use cells with iou > 0 as object targets\n",
    "\n",
    "      # Classification of the class\n",
    "      # Check if we need to do a classification (number of classes > 1)\n",
    "      if ps.size(1) - 5 > 1:\n",
    "        # Hot one class encoding\n",
    "        t = torch.zeros_like(ps[:, 5:], device=device)  # targets\n",
    "        t[range(num_targets), tcls[layer_index]] = 1\n",
    "        # Use the tensor to calculate the BCE loss\n",
    "        lcls += BCEcls(ps[:, 5:], t)  # BCE\n",
    "\n",
    "    # Classification of the objectness the sequel\n",
    "    # Calculate the BCE loss between the on the fly generated target and the network prediction\n",
    "    lobj += BCEobj(layer_predictions[..., 4], tobj) # obj loss\n",
    "\n",
    "  lbox *= 0.05\n",
    "  lobj *= 1.0\n",
    "  lcls *= 0.5\n",
    "\n",
    "  # Merge losses\n",
    "  loss = lbox + lobj + lcls\n",
    "\n",
    "  return loss, torch.cat((lbox, lobj, lcls, loss))\n",
    "\n",
    "\n",
    "def build_targets(p, targets, model):\n",
    "  # Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n",
    "  na, nt = 3, targets.shape[0]  # number of anchors, targets #TODO\n",
    "  tcls, tbox, indices, anch = [], [], [], []\n",
    "  gain = torch.ones(7, device=targets.device)  # normalized to gridspace gain\n",
    "  # Make a tensor that iterates 0-2 for 3 anchors and repeat that as many times as we have target boxes\n",
    "  ai = torch.arange(na, device=targets.device).float().view(na, 1).repeat(1, nt)\n",
    "  # Copy target boxes anchor size times and append an anchor index to each copy the anchor index is also expressed by the new first dimension\n",
    "  targets = torch.cat((targets.repeat(na, 1, 1), ai[:, :, None]), 2)\n",
    "\n",
    "  for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "      # Scale anchors by the yolo grid cell size so that an anchor with the size of the cell would result in 1\n",
    "      anchors = yolo_layer.anchors / yolo_layer.stride\n",
    "      # Add the number of yolo cells in this layer the gain tensor\n",
    "      # The gain tensor matches the collums of our targets (img id, class, x, y, w, h, anchor id)\n",
    "      gain[2:6] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "      # Scale targets by the number of yolo layer cells, they are now in the yolo cell coordinate system\n",
    "      t = targets * gain\n",
    "      # Check if we have targets\n",
    "      if nt:\n",
    "          # Calculate ration between anchor and target box for both width and height\n",
    "          r = t[:, :, 4:6] / anchors[:, None]\n",
    "          # Select the ratios that have the highest divergence in any axis and check if the ratio is less than 4\n",
    "          j = torch.max(r, 1. / r).max(2)[0] < 4  # compare #TODO\n",
    "          # Only use targets that have the correct ratios for their anchors\n",
    "          # That means we only keep ones that have a matching anchor and we loose the anchor dimension\n",
    "          # The anchor id is still saved in the 7th value of each target\n",
    "          t = t[j]\n",
    "      else:\n",
    "          t = targets[0]\n",
    "\n",
    "      # Extract image id in batch and class id\n",
    "      b, c = t[:, :2].long().T\n",
    "      # We isolate the target cell associations.\n",
    "      # x, y, w, h are allready in the cell coordinate system meaning an x = 1.2 would be 1.2 times cellwidth\n",
    "      gxy = t[:, 2:4]\n",
    "      gwh = t[:, 4:6]  # grid wh\n",
    "      # Cast to int to get an cell index e.g. 1.2 gets associated to cell 1\n",
    "      gij = gxy.long()\n",
    "      # Isolate x and y index dimensions\n",
    "      gi, gj = gij.T  # grid xy indices\n",
    "\n",
    "      # Convert anchor indexes to int\n",
    "      a = t[:, 6].long()\n",
    "      # Add target tensors for this yolo layer to the output lists\n",
    "      # Add to index list and limit index range to prevent out of bounds\n",
    "      indices.append((b, a, gj.clamp_(0, gain[3].long() - 1), gi.clamp_(0, gain[2].long() - 1)))\n",
    "      # Add to target box list and convert box coordinates from global grid coordinates to local offsets in the grid cell\n",
    "      tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n",
    "      # Add correct anchor for each target to the list\n",
    "      anch.append(anchors[a])\n",
    "      # Add class for each target to the list\n",
    "      tcls.append(c)\n",
    "\n",
    "  return tcls, tbox, indices, anch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
